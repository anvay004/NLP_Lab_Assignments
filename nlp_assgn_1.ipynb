{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z8RydeyU0I9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "477bd966",
        "outputId": "8899b500-9806-401b-e08c-7f699ddc1713"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac5ba043"
      },
      "source": [
        "## 1. Tokenization\n",
        "Tokenization is the process of breaking down a text into smaller units called tokens. Let's explore different tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f6fa641",
        "outputId": "84130465-0916-4853-d985-bd27f53b7d2e"
      },
      "source": [
        "from nltk.tokenize import word_tokenize, WhitespaceTokenizer, PunktTokenizer, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
        "\n",
        "text = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The user doesn't like MWEs!\"\n",
        "print(f\"Original Text: {text}\\n\")\n",
        "\n",
        "# Whitespace Tokenizer\n",
        "print(\"--- Whitespace Tokenizer ---\")\n",
        "wt = WhitespaceTokenizer()\n",
        "ws_tokens = wt.tokenize(text)\n",
        "print(f\"Tokens: {ws_tokens}\\n\")\n",
        "\n",
        "# Punctuation Tokenizer (using word_tokenize which defaults to TreebankWordTokenizer but handles punctuation)\n",
        "print(\"--- Punctuation Tokenizer ---\")\n",
        "punct_tokens = word_tokenize(text)\n",
        "print(f\"Tokens: {punct_tokens}\\n\")\n",
        "\n",
        "# Treebank Tokenizer\n",
        "print(\"--- Treebank Tokenizer ---\")\n",
        "tt = TreebankWordTokenizer()\n",
        "treebank_tokens = tt.tokenize(text)\n",
        "print(f\"Tokens: {treebank_tokens}\\n\")\n",
        "\n",
        "# Tweet Tokenizer\n",
        "print(\"--- Tweet Tokenizer ---\")\n",
        "tw = TweetTokenizer()\n",
        "tweet_tokens = tw.tokenize(\"This is a #tweet with a @mention and a smiley :) - check it out at http://example.com/ #NLP\")\n",
        "print(f\"Tokens: {tweet_tokens}\\n\")\n",
        "\n",
        "# MWE Tokenizer (Multi-Word Expression Tokenizer)\n",
        "print(\"--- MWE Tokenizer ---\")\n",
        "mwe_tokenizer = MWETokenizer([('natural', 'language'), ('New', 'York')])\n",
        "mwe_text = \"I am learning about natural language processing in New York.\"\n",
        "mwe_tokens = mwe_tokenizer.tokenize(mwe_text.split())\n",
        "print(f\"Tokens: {mwe_tokens}\\n\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The user doesn't like MWEs!\n",
            "\n",
            "--- Whitespace Tokenizer ---\n",
            "Tokens: ['Hello', 'Mr.', 'Smith,', 'how', 'are', 'you', 'doing', 'today?', 'The', 'weather', 'is', 'great,', 'and', 'Python', 'is', 'awesome.', 'The', 'user', \"doesn't\", 'like', 'MWEs!']\n",
            "\n",
            "--- Punctuation Tokenizer ---\n",
            "Tokens: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'user', 'does', \"n't\", 'like', 'MWEs', '!']\n",
            "\n",
            "--- Treebank Tokenizer ---\n",
            "Tokens: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome.', 'The', 'user', 'does', \"n't\", 'like', 'MWEs', '!']\n",
            "\n",
            "--- Tweet Tokenizer ---\n",
            "Tokens: ['This', 'is', 'a', '#tweet', 'with', 'a', '@mention', 'and', 'a', 'smiley', ':)', '-', 'check', 'it', 'out', 'at', 'http://example.com/', '#NLP']\n",
            "\n",
            "--- MWE Tokenizer ---\n",
            "Tokens: ['I', 'am', 'learning', 'about', 'natural_language', 'processing', 'in', 'New', 'York.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70e663f8"
      },
      "source": [
        "## 2. Stemming\n",
        "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem is not necessarily a linguistic root of the word; it is usually an affix that has been removed.\n",
        "\n",
        "Let's apply Porter and Snowball Stemmers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60a5d023",
        "outputId": "fd5c24d2-c047-47fb-dccb-70039d0d64de"
      },
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "words_to_stem = [\"running\", \"runner\", \"runs\", \"easily\", \"fairly\", \"universal\", \"universally\"]\n",
        "\n",
        "# Porter Stemmer\n",
        "print(\"--- Porter Stemmer ---\")\n",
        "ps = PorterStemmer()\n",
        "for word in words_to_stem:\n",
        "    print(f\"{word} -> {ps.stem(word)}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Snowball Stemmer (English)\n",
        "print(\"--- Snowball Stemmer (English) ---\")\n",
        "sbs = SnowballStemmer(\"english\")\n",
        "for word in words_to_stem:\n",
        "    print(f\"{word} -> {sbs.stem(word)}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Snowball Stemmer (another language example, e.g., French)\n",
        "print(\"--- Snowball Stemmer (French) Example ---\")\n",
        "sbs_fr = SnowballStemmer(\"french\")\n",
        "french_words = [\"manger\", \"mangeant\", \"mangé\"]\n",
        "for word in french_words:\n",
        "    print(f\"{word} -> {sbs_fr.stem(word)}\")\n",
        "print(\"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Porter Stemmer ---\n",
            "running -> run\n",
            "runner -> runner\n",
            "runs -> run\n",
            "easily -> easili\n",
            "fairly -> fairli\n",
            "universal -> univers\n",
            "universally -> univers\n",
            "\n",
            "\n",
            "--- Snowball Stemmer (English) ---\n",
            "running -> run\n",
            "runner -> runner\n",
            "runs -> run\n",
            "easily -> easili\n",
            "fairly -> fair\n",
            "universal -> univers\n",
            "universally -> univers\n",
            "\n",
            "\n",
            "--- Snowball Stemmer (French) Example ---\n",
            "manger -> mang\n",
            "mangeant -> mang\n",
            "mangé -> mang\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a8ae43f"
      },
      "source": [
        "## 3. Lemmatization\n",
        "Lemmatization is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word's lemma, or dictionary form. Unlike stemming, lemmatization considers the context and converts the word to its meaningful base form.\n",
        "\n",
        "Let's use the WordNet Lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d181d3a",
        "outputId": "de15cc8e-dedc-4704-8019-950137283dc8"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "words_to_lemmatize = [\"running\", \"runner\", \"runs\", \"ran\", \"better\", \"good\", \"geese\", \"cacti\"]\n",
        "\n",
        "# WordNet Lemmatizer\n",
        "print(\"--- WordNet Lemmatizer ---\")\n",
        "wnl = WordNetLemmatizer()\n",
        "for word in words_to_lemmatize:\n",
        "    print(f\"{word} -> {wnl.lemmatize(word)}\")\n",
        "\n",
        "print(\"\\nNote: Lemmatizer performs better with part-of-speech (POS) tags. Let's see an example with POS tags.\")\n",
        "print(f\"{'better':<10} -> {wnl.lemmatize('better', pos='a')}\") # 'a' for adjective\n",
        "print(f\"{'running':<10} -> {wnl.lemmatize('running', pos='v')}\") # 'v' for verb\n",
        "print(f\"{'runs':<10} -> {wnl.lemmatize('runs', pos='v')}\") # 'v' for verb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- WordNet Lemmatizer ---\n",
            "running -> running\n",
            "runner -> runner\n",
            "runs -> run\n",
            "ran -> ran\n",
            "better -> better\n",
            "good -> good\n",
            "geese -> goose\n",
            "cacti -> cactus\n",
            "\n",
            "Note: Lemmatizer performs better with part-of-speech (POS) tags. Let's see an example with POS tags.\n",
            "better     -> good\n",
            "running    -> run\n",
            "runs       -> run\n"
          ]
        }
      ]
    }
  ]
}